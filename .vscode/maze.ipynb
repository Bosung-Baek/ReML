{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑        |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →       |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"           →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←          |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓        |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑      |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"    →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"    ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←      |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"       ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"       →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"       ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"       ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    action = np.array([[-1, 0], [0, 1], [1, 0], [0, -1]])\n",
    "    select_action_pr = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    pos = np.array([0, 0])\n",
    "\n",
    "    def set_pos(self, position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "\n",
    "    goal_position = [2, 2]\n",
    "\n",
    "    reward_list = [[road, road, road],\n",
    "                   [road, road, road], \n",
    "                   [road, road, goal]]\n",
    "    \n",
    "    reward_list1 = [[\"road\", \"road\", \"road\"],\n",
    "                    [\"road\", \"road\", \"road\"],\n",
    "                    [\"road\", \"road\", \"goal\"]]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)\n",
    "\n",
    "    def move(self, agent:Agent, action):\n",
    "        done = False\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "\n",
    "        elif not(new_pos[0] in range(0, 3)) or not(new_pos[1] in range(0, 3)):\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        \n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0], observation[1]]\n",
    "\n",
    "        return observation, reward, done\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_value_function(env:Environment, agent:Agent, G, max_step, now_step):\n",
    "    gamma = 0.9\n",
    "\n",
    "    if env.reward_list1[agent.pos[0]][ agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    if max_step == now_step:\n",
    "        pos1 = agent.get_pos()\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            observation, reward, done = env.move(agent, i)\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "        return G\n",
    "    else:\n",
    "        pos1 = agent.get_pos()\n",
    "        for i in range(len(agent.action)):\n",
    "            observation, reward, done = env.move(agent, i)\n",
    "            G+= agent.select_action_pr[i]*reward\n",
    "\n",
    "            if done == True:\n",
    "                if not(observation[0] in range(0, 3)) or not(observation[1] in range(0, 3)):\n",
    "                    agent.set_pos(pos1)\n",
    "            \n",
    "            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n",
    "            G+= agent.select_action_pr[i]*gamma*next_v\n",
    "\n",
    "            agent.set_pos(pos1)\n",
    "\n",
    "        return G\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "max_step_number = 0 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.50      |      -2.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -1.50      |      -1.00      |      -1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.00      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.58      |      -2.96      |      -3.46      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.96      |      -2.12      |      -1.68      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.46      |      -1.68      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "max_step_number = 2 total_time = 0.01(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.94      |      -4.23      |      -4.60      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.23      |      -3.09      |      -2.41      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.60      |      -2.41      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "max_step_number = 3 total_time = 0.02(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.13      |      -5.29      |      -5.56      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.29      |      -3.99      |      -3.05      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.56      |      -3.05      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "max_step_number = 4 total_time = 0.07(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.14      |      -6.22      |      -6.38      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.22      |      -4.75      |      -3.61      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.38      |      -3.61      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "max_step_number = 5 total_time = 0.22(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.01      |      -7.01      |      -7.08      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.01      |      -5.42      |      -4.09      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.08      |      -4.09      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "max_step_number = 6 total_time = 0.83(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.76      |      -7.69      |      -7.69      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -6.00      |      -4.51      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -4.51      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 7 total_time = 3.02(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -9.40      |      -8.28      |      -8.20      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.28      |      -6.49      |      -4.87      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.20      |      -4.87      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent()\n",
    "max_step_number = 8\n",
    "time_len = []\n",
    "for max_step in range(max_step_number):\n",
    "    v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    start_time = time.time()\n",
    "    print(v_table)\n",
    "\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            agent.set_pos([i, j])\n",
    "            v_table[i, j] = state_value_function(env, agent, 0, max_step, 0)\n",
    "\n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time, 2)))\n",
    "\n",
    "    show_v_table(np.round(v_table, 2),env)\n",
    "\n",
    "plt.plot(time_len, 'o-k')\n",
    "plt.xlabel('max_down')\n",
    "plt.ylabel('time(s)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_value_function(env:Environment, agent:Agent, act, G, max_step, now_step):\n",
    "    gamma = 0.9\n",
    "    \n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    if max_step == now_step:\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G +=agent.select_action_pr[act]*reward\n",
    "        return G\n",
    "    else:\n",
    "        pos1 = agent.get_pos() # 현재 상태 저장\n",
    "        observation,reward, done = env.move(agent, act)\n",
    "        G +=agent.select_action_pr[act]*reward\n",
    "        \n",
    "        if done == True: \n",
    "            if not(0<=observation[0]<3 and 0<=observation[1]<3): #절벽에 떨어졌을 경우\n",
    "                agent.set_pos(pos1)\n",
    "                \n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n",
    "            G+= agent.select_action_pr[i]*gamma*next_v\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step = 0\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.75       |     -0.75       |     -0.75       |\n",
      "| -0.75     -0.25 | -0.25     -0.25 | -0.25     -0.75 |\n",
      "|     -0.25       |     -0.25       |     -0.25       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.25       |     -0.25       |     -0.25       |\n",
      "| -0.75     -0.25 | -0.25     -0.25 | -0.25     -0.75 |\n",
      "|     -0.25       |     -0.25       |      0.25       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.25       |     -0.25       |      1.00       |\n",
      "| -0.75     -0.25 | -0.25      0.25 |  1.00      1.00 |\n",
      "|     -0.75       |     -0.75       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |      ←  →       |      ←          |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|        ↑        |        ↑        |                 |\n",
      "|           →     |      ←  →       |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|        ↑        |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 1\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.20       |     -1.09       |     -1.20       |\n",
      "| -1.20     -0.59 | -0.70     -0.70 | -0.59     -1.20 |\n",
      "|     -0.59       |     -0.48       |     -0.48       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.70       |     -0.59       |     -0.70       |\n",
      "| -1.09     -0.48 | -0.59     -0.48 | -0.48     -0.98 |\n",
      "|     -0.70       |     -0.48       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.59       |     -0.48       |      1.00       |\n",
      "| -1.20     -0.48 | -0.70      1.15 |  1.00      1.00 |\n",
      "|     -1.20       |     -0.98       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |                 |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |           →     |                 |\n",
      "|                 |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 2\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.55       |     -1.42       |     -1.53       |\n",
      "| -1.55     -0.92 | -1.05     -1.03 | -0.92     -1.53 |\n",
      "|     -0.92       |     -0.73       |     -0.48       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.05       |     -0.92       |     -1.03       |\n",
      "| -1.42     -0.73 | -0.92     -0.48 | -0.73     -0.98 |\n",
      "|     -1.03       |     -0.48       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.92       |     -0.73       |      1.00       |\n",
      "| -1.53     -0.48 | -1.03      1.15 |  1.00      1.00 |\n",
      "|     -1.53       |     -0.98       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |                 |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |           →     |                 |\n",
      "|                 |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 3\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.86       |     -1.70       |     -1.75       |\n",
      "| -1.86     -1.20 | -1.36     -1.25 | -1.20     -1.75 |\n",
      "|     -1.20       |     -0.88       |     -0.61       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.36       |     -1.20       |     -1.25       |\n",
      "| -1.70     -0.88 | -1.20     -0.61 | -0.88     -1.11 |\n",
      "|     -1.25       |     -0.61       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.20       |     -0.88       |      1.00       |\n",
      "| -1.75     -0.61 | -1.25      1.15 |  1.00      1.00 |\n",
      "|     -1.75       |     -1.11       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |                 |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |           →     |                 |\n",
      "|                 |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 4\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.13       |     -1.92       |     -1.94       |\n",
      "| -2.13     -1.42 | -1.63     -1.44 | -1.42     -1.94 |\n",
      "|     -1.42       |     -1.06       |     -0.72       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.63       |     -1.42       |     -1.44       |\n",
      "| -1.92     -1.06 | -1.42     -0.72 | -1.06     -1.22 |\n",
      "|     -1.44       |     -0.72       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.42       |     -1.06       |      1.00       |\n",
      "| -1.94     -0.72 | -1.44      1.15 |  1.00      1.00 |\n",
      "|     -1.94       |     -1.22       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |                 |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |           →     |                 |\n",
      "|                 |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 5\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.35       |     -2.11       |     -2.11       |\n",
      "| -2.35     -1.61 | -1.85     -1.61 | -1.61     -2.11 |\n",
      "|     -1.61       |     -1.21       |     -0.83       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.85       |     -1.61       |     -1.61       |\n",
      "| -2.11     -1.21 | -1.61     -0.83 | -1.21     -1.33 |\n",
      "|     -1.61       |     -0.83       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.61       |     -1.21       |      1.00       |\n",
      "| -2.11     -0.83 | -1.61      1.15 |  1.00      1.00 |\n",
      "|     -2.11       |     -1.33       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |                 |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |           →     |                 |\n",
      "|                 |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 6\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.53       |     -2.27       |     -2.25       |\n",
      "| -2.53     -1.77 | -2.03     -1.75 | -1.77     -2.25 |\n",
      "|     -1.77       |     -1.35       |     -0.92       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.03       |     -1.77       |     -1.75       |\n",
      "| -2.27     -1.35 | -1.77     -0.92 | -1.35     -1.42 |\n",
      "|     -1.75       |     -0.92       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.77       |     -1.35       |      1.00       |\n",
      "| -2.25     -0.92 | -1.75      1.15 |  1.00      1.00 |\n",
      "|     -2.25       |     -1.42       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |                 |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |           →     |                 |\n",
      "|                 |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 7\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.69       |     -2.42       |     -2.37       |\n",
      "| -2.69     -1.92 | -2.19     -1.87 | -1.92     -2.37 |\n",
      "|     -1.92       |     -1.46       |     -1.01       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.19       |     -1.92       |     -1.87       |\n",
      "| -2.42     -1.46 | -1.92     -1.01 | -1.46     -1.51 |\n",
      "|     -1.87       |     -1.01       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.92       |     -1.46       |      1.00       |\n",
      "| -2.37     -1.01 | -1.87      1.15 |  1.00      1.00 |\n",
      "|     -2.37       |     -1.51       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |                 |                 |\n",
      "|        ↓        |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|           →     |           →     |                 |\n",
      "|                 |        ↓        |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑        |\n",
      "|           →     |           →     |      ←  →       |\n",
      "|                 |                 |        ↓        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 재귀적으로 행동의 가치를 계산\n",
    "\n",
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "# 3. 현재부터 max_step 까지 계산\n",
    "max_step_number = 8\n",
    "\n",
    "# 4. 모든 상태에 대해\n",
    "for max_step in range(max_step_number):\n",
    "    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\n",
    "    print(\"max_step = {}\".format(max_step))\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            # 4.2 모든 행동에 대해\n",
    "            for action in range(len(agent.action)):\n",
    "                # 4.2.1 에이전트의 위치를 초기화\n",
    "                agent.set_pos([i,j])\n",
    "                # 4.2.2 현재 위치에서 행동 가치를 계산\n",
    "                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n",
    "\n",
    "    q = np.round(q_table,2)\n",
    "    print(\"Q - table\")\n",
    "    show_q_table(q, env)\n",
    "    print(\"High actions Arrow\")\n",
    "    show_q_table_arrow(q,env)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "k = 1\n",
    "\n",
    "while(True):\n",
    "    delta = 0\n",
    "    \n",
    "    temp_v = copy.deepcopy(v_table)\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            G = 0\n",
    "            \n",
    "            for action in range(4):\n",
    "                agent.set_pos([i, j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                \n",
    "                G += agent.select_action_pr[action]*(reward + gamma * v_table[observation[0], observation[1]])\n",
    "                \n",
    "            v_table[i, j] = G\n",
    "                \n",
    "    delta = np.max(np.abs(temp_v-v_table))\n",
    "\n",
    "    k+=1\n",
    "    if delta<0.000001:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, agent, v_table, policy):\n",
    "    gamma = 0.9\n",
    "\n",
    "    v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    k = 1\n",
    "\n",
    "    while(True):\n",
    "        delta = 0\n",
    "\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                agent.set_pos([i, j])\n",
    "                action = policy[i, j]\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                v_table[i, j] = reward + gamma * v_table[observation[0], observation[1]]\n",
    "\n",
    "        delta = np.max(np.abs(temp_v-v_table))\n",
    "        k+=1\n",
    "        if delta<0.000001:\n",
    "            break\n",
    "    return v_table, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, agent, v_table, policy):\n",
    "    policyStable = True\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            old_action = policy[i, j]\n",
    "            \n",
    "            temp_action = 0\n",
    "            temp_value = -1e+10\n",
    "            for action in range(4):\n",
    "                agent.set_pos([i, j])\n",
    "                observation, reward, done = env.move(agent,action)\n",
    "                if temp_value <reward + gamma * v_table[observation[0], observation[1]]:\n",
    "                    temp_action = action\n",
    "                    temp_value = reward + gamma*v_table[observation[0], observation[1]]\n",
    "            if old_action != temp_action:\n",
    "                policyStable = False\n",
    "            policy[i, j] = temp_action\n",
    "    return policy, policyStable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_time = 7.998910665512085\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "v_table = np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "policy = np.random.randint(0, 4, (env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "max_iter_number = 20000\n",
    "for iter_number in range(max_iter_number):\n",
    "    \n",
    "    v_table, delta = policy_evaluation(env, agent, v_table, policy)\n",
    "    \n",
    "    policy, policyStable = policy_improvement(env, agent, v_table, policy)\n",
    "    \n",
    "    if(policyStable == True):\n",
    "        break\n",
    "    k+=1\n",
    "print(\"total_time = {}\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_optimal_value(env, agent, v_table):\n",
    "    k = 0\n",
    "    gamma = 0.9\n",
    "    while(True):\n",
    "        delta = 0\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                temp = -1e+10\n",
    "\n",
    "                for action in range(len(agent.action)):\n",
    "                    agent.set_pos([i, j])\n",
    "                    observation, reward, done = env.move(agent, action)\n",
    "\n",
    "                    if temp < reward + gamma*v_table[observation[0], observation[1]]:\n",
    "                        temp = reward + gamma*v_table[observation[0], observation[1]]\n",
    "                v_table[i, j] = temp\n",
    "        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])\n",
    "\n",
    "        if delta < 0.0000001:\n",
    "            break\n",
    "        k += 1\n",
    "        return v_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(env:Environment, agent:Agent, v_table, optimal_policy):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            temp = -1e+10\n",
    "\n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i, j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                if temp<reward + gamma * v_table[observation[0], observation[1]]:\n",
    "                    optimal_policy[i, j] = action\n",
    "                    temp = reward + gamma * v_table[observation[0], observation[1]]\n",
    "    return optimal_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start Value iteration\n",
      "optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       →         |       ↓         |       ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       →         |       →         |       ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       →         |       →         |       ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "v_table = np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "print(\"start Value iteration\")\n",
    "v_table = finding_optimal_value(env, agent, v_table)\n",
    "optimal_policy = policy_extraction(env, agent, v_table, optimal_policy)\n",
    "print(\"optimal policy\")\n",
    "show_policy(optimal_policy, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env:Environment, agent:Agent, first_visit):\n",
    "    gamma = 0.09\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    i = np.random.randint(0, env.reward.shape[0])\n",
    "    j = np.random.randint(0, env.reward.shape[1])\n",
    "    agent.set_pos([i, j])\n",
    "\n",
    "    G = 0\n",
    "\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        action = np.random.randint(0, len(agent.action))\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        if first_visit:\n",
    "            if visit[pos[0], pos[1]] == 0:\n",
    "                G += gamma**(step) * reward\n",
    "                visit[pos[0], pos[1]] = 1\n",
    "                step +=1\n",
    "                episode.append((pos, action, reward))\n",
    "        else:\n",
    "            G += gamma ** (step) * reward\n",
    "            step += 1\n",
    "            episode.append((pos, action, reward))\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    return i, j, G, episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start every visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:49<00:00, 2022.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.07      |      -1.62      |      -2.06      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -1.62      |      -1.12      |      -1.06      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.05      |      -1.05      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_start_count(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11302.00      |   11148.00      |   11021.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11077.00      |   11027.00      |   11109.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11151.00      |   11075.00      |   11090.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_success_pr(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.04      |       0.09      |       0.10      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.09      |       0.20      |       0.33      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.10      |       0.33      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "v_table = np.zeros((3, 3))\n",
    "v_start = np.zeros((3, 3))\n",
    "v_success = np.zeros((3, 3))\n",
    "\n",
    "Return_s = [[[] for j in range(3)] for i in range(3)]\n",
    "\n",
    "max_episode = 100000\n",
    "\n",
    "first_visit = False\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else:\n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    i, j, G, episode = generate_episode(env, agent, first_visit)\n",
    "    Return_s[i][j].append(G)\n",
    "\n",
    "    episode_count = len(Return_s[i][j])\n",
    "\n",
    "    total_G = np.sum(Return_s[i][j])\n",
    "\n",
    "    v_table[i, j] = total_G / episode_count\n",
    "\n",
    "    if episode[-1][2] == 1:\n",
    "        v_success[i, j] += 1\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        v_start[i, j] = len(Return_s[i][j])\n",
    "\n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(v_table, 2), env)\n",
    "print(\"V_start_count(s)\")\n",
    "show_v_table(np.round(v_start, 2), env)\n",
    "print(\"V_success_pr(s)\")\n",
    "show_v_table(np.round(v_success/v_start, 2), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
