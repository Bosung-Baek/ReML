{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 20186.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -12.59      |     -11.01      |     -10.12      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -10.85      |      -8.53      |      -5.80      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -9.68      |      -6.06      |       3.48      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TD(0) prediction\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "V = np.zeros((3, 3))\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "print(\"start TD(0) prediction\")\n",
    "\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i, j])\n",
    "\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        action = np.random.randint(0, 4)\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        V[pos[0], pos[1]] += alpha * (reward + gamma * V[observation[0], observation[1]] - V[pos[0], pos[1]])\n",
    "        if done:\n",
    "            break\n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(V, 2), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon-greedy\n",
    "def e_greedy(Q_table, agent, epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    greedy_action = np.argmax(Q_table[pos[0], pos[1], :])\n",
    "    pr = np.zeros(4)\n",
    "    for i in range(len(agent.action)):\n",
    "        if i == greedy_action:\n",
    "            pr[i] = 1- epsilon + epsilon/4\n",
    "        else:\n",
    "            pr[i] = epsilon/4\n",
    "    return np.random.choice(4, p=pr)\n",
    "#greedy\n",
    "def greedy(Q_table, agent):\n",
    "    pos = agent.get_pos()\n",
    "    return np.argmax(Q_table[pos[0], pos[1], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3897.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA : Q(s, a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -9.20       |     -7.30       |     -5.16       |\n",
      "| -9.71     -5.47 | -7.55     -4.38 | -5.91     -6.55 |\n",
      "|     -5.24       |     -3.63       |      0.40       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -6.74       |     -4.63       |     -4.46       |\n",
      "| -7.34     -3.18 | -5.42      3.59 | -3.82     -1.42 |\n",
      "|     -3.02       |     -1.13       |      9.70       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -5.06       |     -3.86       |      9.63       |\n",
      "| -6.74     -1.92 | -4.77      9.68 |  9.66      9.68 |\n",
      "|     -5.29       |     -1.65       |      9.69       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "SARSA : Optimal Policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TD(0) control : SARSA\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#Q-table\n",
    "Q_table = np.random.rand(3, 3, 4)\n",
    "#Q(terminal state) = 0\n",
    "Q_table[2, 2, :] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "print(\"start SARSA\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "#every episode\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta = 0\n",
    "    #initialize s\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i, j])\n",
    "    temp = 0\n",
    "\n",
    "    #choose action\n",
    "    action = e_greedy(Q_table, agent, epsilon)\n",
    "    #every step\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        #move\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        #choose action\n",
    "        action_next = e_greedy(Q_table, agent, epsilon)\n",
    "        #update Q-table\n",
    "        Q_table[pos[0], pos[1], action] += alpha * (reward + gamma * Q_table[observation[0], observation[1], action_next] - Q_table[pos[0], pos[1], action])\n",
    "        #update s\n",
    "        agent.set_pos(observation)\n",
    "        #update action\n",
    "        action = action_next\n",
    "        #update delta\n",
    "        delta += reward\n",
    "        #terminal state\n",
    "        if done:\n",
    "            break\n",
    "#optimal policy\n",
    "optimal_policy = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimal_policy[i, j] = np.argmax(Q_table[i, j, :])\n",
    "\n",
    "print(\"SARSA : Q(s, a)\")\n",
    "show_q_table(np.round(Q_table, 2), env)\n",
    "print(\"SARSA : Optimal Policy\")\n",
    "show_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4436.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning : Q(s, a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      1.10       |      2.56       |      4.17       |\n",
      "|  1.10      4.56 |  3.10      6.17 |  4.55      4.17 |\n",
      "|      4.56       |      6.18       |      7.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      3.10       |      4.56       |      6.17       |\n",
      "|  2.56      6.18 |  4.56      7.98 |  6.18      5.97 |\n",
      "|      6.18       |      7.97       |      9.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      4.56       |      6.18       |      9.97       |\n",
      "|  4.17      7.97 |  6.17      9.97 |  9.97      9.97 |\n",
      "|      4.17       |      5.97       |      9.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Q-learning : Optimal Policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ←         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TD(0) control : Q-learning\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#initialize Q-table\n",
    "Q_table = np.random.rand(3, 3, 4)\n",
    "#Q(terminal state) = 0\n",
    "Q_table[2, 2, :] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "print(\"start Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "#every episode\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta = 0\n",
    "    #initialize s\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i, j])\n",
    "\n",
    "    #every step\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        #choose action\n",
    "        action = e_greedy(Q_table, agent, epsilon)\n",
    "        #move\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        #next action\n",
    "        next_action = greedy(Q_table, agent)\n",
    "        #update Q-table\n",
    "        Q_table[pos[0], pos[1], action] += alpha * (reward + gamma * Q_table[observation[0], observation[1], next_action] - Q_table[pos[0], pos[1], action])\n",
    "        #update s\n",
    "        agent.set_pos(observation)\n",
    "        #update delta\n",
    "        delta += reward\n",
    "        #terminal state\n",
    "        if done:\n",
    "            break\n",
    "#optimal policy\n",
    "optimal_policy = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimal_policy[i, j] = np.argmax(Q_table[i, j, :])\n",
    "\n",
    "print(\"Q-learning : Q(s, a)\")\n",
    "show_q_table(np.round(Q_table, 2), env)\n",
    "print(\"Q-learning : Optimal Policy\")\n",
    "show_policy(optimal_policy, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start double Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4903.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double Q-learning : Q1(s, a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      1.12       |      2.58       |      4.16       |\n",
      "|  1.12      4.58 |  3.12      6.20 |  4.54      4.12 |\n",
      "|      4.58       |      6.20       |      8.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      3.11       |      4.58       |      6.20       |\n",
      "|  2.53      6.11 |  4.58      8.00 |  6.20      6.00 |\n",
      "|      6.20       |      8.00       |     10.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      4.40       |      6.16       |     10.00       |\n",
      "|  4.08      8.00 |  6.19     10.00 | 10.00     10.00 |\n",
      "|      4.14       |      5.96       |     10.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "double Q-learning : Q2(s, a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      1.12       |      2.58       |      4.11       |\n",
      "|  1.12      4.58 |  3.12      6.20 |  4.50      4.08 |\n",
      "|      4.58       |      6.20       |      8.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      3.10       |      4.58       |      6.20       |\n",
      "|  2.52      6.19 |  4.58      8.00 |  6.20      6.00 |\n",
      "|      6.20       |      8.00       |     10.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      4.48       |      6.17       |     10.00       |\n",
      "|  4.04      8.00 |  6.12     10.00 | 10.00     10.00 |\n",
      "|      4.18       |      5.99       |     10.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "double Q-learning : Q(s, a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      2.24       |      5.16       |      8.28       |\n",
      "|  2.24      9.16 |  6.24     12.40 |  9.04      8.20 |\n",
      "|      9.16       |     12.40       |     16.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      6.21       |      9.16       |     12.40       |\n",
      "|  5.05     12.30 |  9.16     16.00 | 12.40     12.00 |\n",
      "|     12.40       |     16.00       |     20.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      8.88       |     12.34       |     20.00       |\n",
      "|  8.12     16.00 | 12.31     20.00 | 20.00     20.00 |\n",
      "|      8.32       |     11.95       |     20.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "double Q-learning : Optimal Policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#double Q-learning\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#initialize Q-table\n",
    "Q1_table = np.random.rand(3, 3, 4)\n",
    "Q2_table = np.random.rand(3, 3, 4)\n",
    "#Q(terminal state) = 0\n",
    "Q1_table[2, 2, :] = 0\n",
    "Q2_table[2, 2, :] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 10\n",
    "\n",
    "print(\"start double Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.3\n",
    "\n",
    "#every episode\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta = 0\n",
    "    #initialize s\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i, j])\n",
    "\n",
    "    #every step\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        #choose action\n",
    "        action = e_greedy(Q1_table, agent, epsilon)\n",
    "        #move\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        p = np.random.random()\n",
    "        #update Q-table\n",
    "        if(p<0.5):\n",
    "            next_action = greedy(Q1_table, agent)\n",
    "            Q1_table[pos[0], pos[1], action] += alpha * (reward + gamma * Q2_table[observation[0], observation[1], next_action] - Q1_table[pos[0], pos[1], action])\n",
    "        else:\n",
    "            next_action = greedy(Q2_table, agent)\n",
    "            Q2_table[pos[0], pos[1], action] += alpha * (reward + gamma * Q1_table[observation[0], observation[1], next_action] - Q2_table[pos[0], pos[1], action])\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "optimal_policy = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimal_policy[i, j] = np.argmax(Q1_table[i, j, :])\n",
    "print(\"double Q-learning : Q1(s, a)\")\n",
    "show_q_table(np.round(Q1_table, 2), env)\n",
    "print(\"double Q-learning : Q2(s, a)\")\n",
    "show_q_table(np.round(Q2_table, 2), env)\n",
    "print(\"double Q-learning : Q(s, a)\")\n",
    "show_q_table(np.round(Q1_table + Q2_table, 2), env)\n",
    "print(\"double Q-learning : Optimal Policy\")\n",
    "show_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start actor-critic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2153.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor-critic : V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.81      |      -4.33      |      -2.75      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.22      |      -2.08      |       2.15      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.06      |       1.76      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "actor-critic : Policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.02       |      0.00       |      0.00       |\n",
      "|  0.00      0.49 |  0.05      0.36 |  0.13      0.08 |\n",
      "|      0.48       |      0.58       |      0.79       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.09       |      0.00       |      0.00       |\n",
      "|  0.00      0.55 |  0.08      0.48 |  0.11      0.11 |\n",
      "|      0.36       |      0.44       |      0.78       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.08       |      0.04       |      0.22       |\n",
      "|  0.00      0.89 |  0.04      0.92 |  0.32      0.34 |\n",
      "|      0.03       |      0.00       |      0.12       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "actor-critic : Optimal Policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      →         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#actor-critic\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "#p(s, a), V(s) random\n",
    "V = np.random.rand(3, 3)\n",
    "policy = np.random.rand(3, 3, 4)\n",
    "\n",
    "#policy sum = 1\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        policy[i, j, :] /= np.sum(policy[i, j, :])\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start actor-critic\")\n",
    "alpha = 0.1\n",
    "\n",
    "#every episode\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    #initialize s\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i, j])\n",
    "\n",
    "    #every step\n",
    "    for k in range(max_step):\n",
    "        #action\n",
    "        pos = agent.get_pos()\n",
    "        pr = np.zeros(4)\n",
    "        #gibbs softmax function\n",
    "        for i in range(4):\n",
    "            pr[i] = np.exp(policy[pos[0], pos[1], i])/np.sum(np.exp(policy[pos[0], pos[1], :]))\n",
    "        #choose action\n",
    "        action = np.random.choice(4, p=pr)\n",
    "        #move\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        #critic learning\n",
    "        td_error = reward + gamma * V[observation[0], observation[1]] - V[pos[0], pos[1]]\n",
    "        #V(s) = V(s) + alpha * td_error\n",
    "        V[pos[0], pos[1]] += alpha * td_error\n",
    "        #actor learning\n",
    "        policy[pos[0], pos[1], action] += td_error * 0.01\n",
    "        #minus to plus\n",
    "        if np.min(policy[pos[0], pos[1], :]) < 0:\n",
    "            policy[pos[0], pos[1], :] -= np.min(policy[pos[0], pos[1], :])\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                policy[i, j, :] /= np.sum(policy[i, j, :])\n",
    "        #s = final state\n",
    "        if done:\n",
    "            break\n",
    "#optimal policy\n",
    "optimal_policy = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimal_policy[i, j] = np.argmax(policy[i, j, :])\n",
    "print(\"actor-critic : V(s)\")\n",
    "show_v_table(np.round(V, 2), env)\n",
    "print(\"actor-critic : Policy\")\n",
    "show_q_table(np.round(policy, 2), env)\n",
    "print(\"actor-critic : Optimal Policy\")\n",
    "show_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) function approximation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 11933.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(0) function approximation : V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -13.92      |     -12.68      |     -11.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -12.26      |     -11.02      |      -9.78      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -10.60      |      -9.36      |      -8.12      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TD(0) function approximation\n",
    "np.random.seed(1)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#initialize\n",
    "#w[0] + w[1]*x1 + w[2]*x2\n",
    "w = np.random.rand(3)\n",
    "w -=0.5\n",
    "\n",
    "v_table = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        v_table[i, j] = np.dot(w, [1, i, j])\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "alpha = 0.01\n",
    "epsilon = 0.3\n",
    "print(\"start TD(0) function approximation\")\n",
    "#every episode\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta = 0\n",
    "    #initialize s\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i, j])\n",
    "    temp = 0\n",
    "\n",
    "    #every step\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        #choose action randomly\n",
    "        action = np.random.choice(4)\n",
    "        #move\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        now_v = 0\n",
    "        next_v = 0\n",
    "\n",
    "        #get now_v and next_v with V(s|w)\n",
    "        now_v = w[0] + np.dot(w[1:], pos)\n",
    "        next_v = w[0] + np.dot(w[1:], observation)\n",
    "\n",
    "        #w = w+alpha*(reward+gamma*next_v-now_v)*x\n",
    "        w[0] = w[0] + alpha * (reward + gamma * next_v - now_v)\n",
    "        w[1] = w[1] + alpha * (reward + gamma * next_v - now_v) * pos[0]\n",
    "        w[2] = w[2] + alpha * (reward + gamma * next_v - now_v) * pos[1]\n",
    "\n",
    "        #s = final state\n",
    "        if done:\n",
    "            break\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        v_table[i, j] = np.dot(w, [1, i, j])\n",
    "print(\"TD(0) function approximation : V(s)\")\n",
    "show_v_table(np.round(v_table, 2), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : function approximation Q-learning : Q(s, a|w)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.05       |      0.15       |      0.25       |\n",
      "| -0.12      0.04 | -0.09      0.19 | -0.06      0.34 |\n",
      "|     -0.06       |      0.40       |      0.86       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.26       |      0.37       |      0.47       |\n",
      "|  0.18     -0.03 |  0.20      0.11 |  0.23      0.26 |\n",
      "|      0.33       |      0.79       |      1.26       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.48       |      0.58       |      0.68       |\n",
      "|  0.47     -0.11 |  0.50      0.04 |  0.52      0.18 |\n",
      "|      0.72       |      1.18       |      1.65       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Before : function approximation Q-learning : Policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↑         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "initial w\n",
      "w = [[ 0.05  0.22  0.1 ]\n",
      " [ 0.04 -0.08  0.15]\n",
      " [-0.06  0.39  0.46]\n",
      " [-0.12  0.29  0.03]]\n",
      "\n",
      "start function approximation Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:58<00:00, 1712.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After : function approximation Q-learning : Q(s, a|w)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -9.44       |     -9.07       |     -8.69       |\n",
      "| -7.18      4.76 | -3.16      5.53 |  0.86      6.31 |\n",
      "|      4.68       |      6.72       |      8.77       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -8.07       |     -7.69       |     -7.32       |\n",
      "| -2.28      6.69 |  1.74      7.47 |  5.75      8.25 |\n",
      "|      5.42       |      7.46       |      9.50       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -6.69       |     -6.32       |     -5.95       |\n",
      "|  2.62      8.63 |  6.63      9.41 | 10.65     10.18 |\n",
      "|      6.15       |      8.20       |     10.24       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "After : function approximation Q-learning : Policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ←         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "final w\n",
      "w = [[-9.44  1.37  0.37]\n",
      " [ 4.76  1.94  0.78]\n",
      " [ 4.68  0.74  2.04]\n",
      " [-7.18  4.9   4.02]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Q-learning function approximation\n",
    "np.random.seed(0)\n",
    "#initialize\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "#q(s, a|w) <- differentiable function\n",
    "#w <- random value\n",
    "w = np.random.rand(len(agent.action), env.reward.shape[0])\n",
    "w -= 0.5\n",
    "FA_Q_table = np.zeros((3, 3, 4))\n",
    "#function save to table\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        for k in range(4):\n",
    "            FA_Q_table[i, j, k] = np.dot(w[k, :], [1, i, j])\n",
    "#get optimal policy\n",
    "optimal_policy = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimal_policy[i, j] = np.argmax(FA_Q_table[i, j, :])\n",
    "\n",
    "print(\"Before : function approximation Q-learning : Q(s, a|w)\")\n",
    "show_q_table(np.round(FA_Q_table, 2), env)\n",
    "print()\n",
    "print(\"Before : function approximation Q-learning : Policy\")\n",
    "show_policy(optimal_policy, env)\n",
    "print()\n",
    "print(\"initial w\")\n",
    "print(\"w = {}\".format(np.round(w, 2)))\n",
    "print()\n",
    "\n",
    "max_episode = 100000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start function approximation Q-learning\")\n",
    "alpha = 0.01\n",
    "\n",
    "#every episode\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    #initial state\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i, j])\n",
    "    #every step\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        #choose action with gibbs softmax function\n",
    "        action = np.zeros(4)\n",
    "        for act in range(4):\n",
    "            action[act] = np.dot(w[act, :], [1, pos[0], pos[1]])\n",
    "        pr = np.zeros(4)\n",
    "        for i in range(4):\n",
    "            pr[i] = np.exp(action[i])/np.sum(np.exp(action))\n",
    "        \n",
    "        action = np.random.choice(4, p=pr)\n",
    "        #move\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        #choose target policy action : greedy\n",
    "        next_act = np.zeros(4)\n",
    "        for act in range(4):\n",
    "            next_act[act] = np.dot(w[act, 1:], observation) + w[act, 0]\n",
    "        best_action = np.argmax(next_act)\n",
    "\n",
    "        now_q = np.dot(w[action, 1:], pos) + w[action, 0]\n",
    "        next_q = np.dot(w[best_action, 1:], observation) + w[best_action, 0]\n",
    "        #w = w+alpha*(reward+gamma*next_q-now_q)*x\n",
    "        w[action, 0] = w[action, 0] + alpha * (reward + gamma * next_q - now_q)\n",
    "        w[action, 1] = w[action, 1] + alpha * (reward + gamma * next_q - now_q) * pos[0]\n",
    "        w[action, 2] = w[action, 2] + alpha * (reward + gamma * next_q - now_q) * pos[1]\n",
    "        #s = final state\n",
    "        if done:\n",
    "            break\n",
    "#FA_Q = np.zeros((4, 3, 3))\n",
    "#function save to table\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        for k in range(4):\n",
    "            FA_Q_table[i, j, k] = np.dot(w[k, :], [1, i, j])\n",
    "\n",
    "#optimal policy\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimal_policy[i, j] = np.argmax(FA_Q_table[i, j, :])\n",
    "print(\"After : function approximation Q-learning : Q(s, a|w)\")\n",
    "show_q_table(np.round(FA_Q_table, 2), env)\n",
    "print()\n",
    "print(\"After : function approximation Q-learning : Policy\")\n",
    "show_policy(optimal_policy, env)\n",
    "print()\n",
    "print(\"final w\")\n",
    "print(\"w = {}\".format(np.round(w, 2)))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
